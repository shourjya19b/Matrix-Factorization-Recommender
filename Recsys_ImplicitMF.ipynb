{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMW-JH4rX4yD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "0O_vy4Tm4bWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_df_raw = pl.read_csv(\"users.dat\", has_header=False)\n",
        "\n",
        "users_df = users_df_raw.select(pl.col(\"column_1\").str.split_exact(\"::\",4).alias(\"columns\")).unnest(\"columns\")\n",
        "\n",
        "users_df = users_df.rename({\"field_0\": \"user_id\",\n",
        "                            \"field_1\": \"gender\",\n",
        "                            \"field_2\": \"age\",\n",
        "                            \"field_3\": \"job\",\n",
        "                            \"field_4\": \"zipcode\"})\n",
        "\n",
        "users_df = users_df.with_columns(pl.col(\"user_id\").cast(pl.Int32),\n",
        "                                 pl.col(\"age\").cast(pl.Int32),\n",
        "                                 pl.col(\"job\").cast(pl.Int32))\n",
        "\n",
        "print(users_df.shape)\n",
        "display(users_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "zIUzWFhdZ3Wa",
        "outputId": "e013b383-7861-4ea6-f523-dfe83c45f7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6040, 5)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "shape: (5, 5)\n",
              "┌─────────┬────────┬─────┬─────┬─────────┐\n",
              "│ user_id ┆ gender ┆ age ┆ job ┆ zipcode │\n",
              "│ ---     ┆ ---    ┆ --- ┆ --- ┆ ---     │\n",
              "│ i32     ┆ str    ┆ i32 ┆ i32 ┆ str     │\n",
              "╞═════════╪════════╪═════╪═════╪═════════╡\n",
              "│ 1       ┆ F      ┆ 1   ┆ 10  ┆ 48067   │\n",
              "│ 2       ┆ M      ┆ 56  ┆ 16  ┆ 70072   │\n",
              "│ 3       ┆ M      ┆ 25  ┆ 15  ┆ 55117   │\n",
              "│ 4       ┆ M      ┆ 45  ┆ 7   ┆ 02460   │\n",
              "│ 5       ┆ M      ┆ 25  ┆ 20  ┆ 55455   │\n",
              "└─────────┴────────┴─────┴─────┴─────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>gender</th><th>age</th><th>job</th><th>zipcode</th></tr><tr><td>i32</td><td>str</td><td>i32</td><td>i32</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;F&quot;</td><td>1</td><td>10</td><td>&quot;48067&quot;</td></tr><tr><td>2</td><td>&quot;M&quot;</td><td>56</td><td>16</td><td>&quot;70072&quot;</td></tr><tr><td>3</td><td>&quot;M&quot;</td><td>25</td><td>15</td><td>&quot;55117&quot;</td></tr><tr><td>4</td><td>&quot;M&quot;</td><td>45</td><td>7</td><td>&quot;02460&quot;</td></tr><tr><td>5</td><td>&quot;M&quot;</td><td>25</td><td>20</td><td>&quot;55455&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df_raw = pl.read_csv(\"movies.dat\",encoding=\"utf8-lossy\",truncate_ragged_lines=True,has_header=False)\n",
        "\n",
        "movies_df = movies_df_raw.select(pl.col(\"column_1\").str.split_exact(\"::\",2).alias(\"columns\")).unnest(\"columns\")\n",
        "\n",
        "movies_df = movies_df.rename({\"field_0\": \"movie_id\",\n",
        "                              \"field_1\": \"title\",\n",
        "                              \"field_2\": \"genres\"})\n",
        "\n",
        "movies_df = movies_df.with_columns(pl.col(\"movie_id\").cast(pl.Int32))\n",
        "\n",
        "print(movies_df.shape)\n",
        "display(movies_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "XyWxiYoHaNT6",
        "outputId": "7c14142e-69cf-4616-a846-88c70f7fbce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3883, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "shape: (5, 3)\n",
              "┌──────────┬─────────────────────────────────┬──────────────────────────────┐\n",
              "│ movie_id ┆ title                           ┆ genres                       │\n",
              "│ ---      ┆ ---                             ┆ ---                          │\n",
              "│ i32      ┆ str                             ┆ str                          │\n",
              "╞══════════╪═════════════════════════════════╪══════════════════════════════╡\n",
              "│ 1        ┆ Toy Story (1995)                ┆ Animation|Children's|Comedy  │\n",
              "│ 2        ┆ Jumanji (1995)                  ┆ Adventure|Children's|Fantasy │\n",
              "│ 3        ┆ Grumpier Old Men (1995)         ┆ Comedy|Romance               │\n",
              "│ 4        ┆ Waiting to Exhale (1995)        ┆ Comedy|Drama                 │\n",
              "│ 5        ┆ Father of the Bride Part II (1… ┆ Comedy                       │\n",
              "└──────────┴─────────────────────────────────┴──────────────────────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movie_id</th><th>title</th><th>genres</th></tr><tr><td>i32</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;Toy Story (1995)&quot;</td><td>&quot;Animation|Children&#x27;s|Comedy&quot;</td></tr><tr><td>2</td><td>&quot;Jumanji (1995)&quot;</td><td>&quot;Adventure|Children&#x27;s|Fantasy&quot;</td></tr><tr><td>3</td><td>&quot;Grumpier Old Men (1995)&quot;</td><td>&quot;Comedy|Romance&quot;</td></tr><tr><td>4</td><td>&quot;Waiting to Exhale (1995)&quot;</td><td>&quot;Comedy|Drama&quot;</td></tr><tr><td>5</td><td>&quot;Father of the Bride Part II (1…</td><td>&quot;Comedy&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df_raw = pl.read_csv(\"ratings.dat\", has_header=False)\n",
        "\n",
        "ratings_df = ratings_df_raw.select(pl.col(\"column_1\").str.split_exact(\"::\",3).alias(\"columns\")).unnest(\"columns\")\n",
        "\n",
        "ratings_df = ratings_df.rename({\"field_0\": \"user_id\",\n",
        "                                \"field_1\": \"movie_id\",\n",
        "                                \"field_2\": \"rating\",\n",
        "                                \"field_3\": \"timestamp\"})\n",
        "\n",
        "ratings_df = ratings_df.with_columns(pl.col(\"user_id\").cast(pl.Int32),\n",
        "                                     pl.col(\"movie_id\").cast(pl.Int32),\n",
        "                                     pl.col(\"rating\").cast(pl.Int32))\n",
        "\n",
        "print(ratings_df.shape)\n",
        "display(ratings_df.head())"
      ],
      "metadata": {
        "id": "ouqKlRlth6U2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "4ffa2418-de0b-4e19-a162-909785db28d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000209, 4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "shape: (5, 4)\n",
              "┌─────────┬──────────┬────────┬───────────┐\n",
              "│ user_id ┆ movie_id ┆ rating ┆ timestamp │\n",
              "│ ---     ┆ ---      ┆ ---    ┆ ---       │\n",
              "│ i32     ┆ i32      ┆ i32    ┆ str       │\n",
              "╞═════════╪══════════╪════════╪═══════════╡\n",
              "│ 1       ┆ 1193     ┆ 5      ┆ 978300760 │\n",
              "│ 1       ┆ 661      ┆ 3      ┆ 978302109 │\n",
              "│ 1       ┆ 914      ┆ 3      ┆ 978301968 │\n",
              "│ 1       ┆ 3408     ┆ 4      ┆ 978300275 │\n",
              "│ 1       ┆ 2355     ┆ 5      ┆ 978824291 │\n",
              "└─────────┴──────────┴────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>movie_id</th><th>rating</th><th>timestamp</th></tr><tr><td>i32</td><td>i32</td><td>i32</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>1193</td><td>5</td><td>&quot;978300760&quot;</td></tr><tr><td>1</td><td>661</td><td>3</td><td>&quot;978302109&quot;</td></tr><tr><td>1</td><td>914</td><td>3</td><td>&quot;978301968&quot;</td></tr><tr><td>1</td><td>3408</td><td>4</td><td>&quot;978300275&quot;</td></tr><tr><td>1</td><td>2355</td><td>5</td><td>&quot;978824291&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checks"
      ],
      "metadata": {
        "id": "J2S5dYhq4nc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_unique_users=users_df.select('user_id').unique().to_series(0).to_list()\n",
        "users_rated_movies=ratings_df.select('user_id').unique().to_series(0).to_list()\n",
        "users_not_rated_movies=list(set(all_unique_users)-set(users_rated_movies))\n",
        "\n",
        "print(len(users_rated_movies))\n",
        "print(len(users_not_rated_movies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFR4ZC2Lj_R-",
        "outputId": "56cb473b-d0f6-49fa-d00a-84bef9d05d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6040\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_unique_movies=movies_df.select('movie_id').unique().to_series(0).to_list()\n",
        "rated_movies=ratings_df.select('movie_id').unique().to_series(0).to_list()\n",
        "unrated_movies=list(set(all_unique_users)-set(users_rated_movies))\n",
        "\n",
        "print(len(rated_movies))\n",
        "print(len(unrated_movies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZwTEqtTn8TQ",
        "outputId": "cf567772-8b4d-407b-c298-432407c2ba4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3706\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=ratings_df.to_pandas()\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Y3q452qA8V",
        "outputId": "42ccd3dd-8ab5-429c-bc98-3f844424ef2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train and test data\n",
        "np.random.seed(42)\n",
        "mask = np.random.rand(len(data)) < 0.8\n",
        "train = data[mask].copy()\n",
        "test = data[~mask].copy()\n",
        "\n",
        "print(len(train))\n",
        "print(len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gUVWiUzoQpU",
        "outputId": "41b5e66f-f9f1-4f97-94ff-5bb776219fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "799910\n",
            "200299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def proc_col(col, train_col=None):\n",
        "    \"\"\"Encodes a pandas column with continuous ids.\n",
        "    \"\"\"\n",
        "    if train_col is not None:\n",
        "        uniq = train_col.unique()\n",
        "    else:\n",
        "        uniq = col.unique()\n",
        "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
        "\n",
        "    return name2idx, np.array([name2idx.get(x, -1) for x in col]), len(uniq)"
      ],
      "metadata": {
        "id": "J0kzwuavjrFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(df, train=None):\n",
        "    \"\"\" Encodes ratings data with continous user and movie ids.\n",
        "    If train is provided, encodes df with the same encoding as train.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for col_name in [\"user_id\", \"movie_id\"]:\n",
        "        train_col = None\n",
        "        if train is not None:\n",
        "            train_col = train[col_name]\n",
        "        _,col,_ = proc_col(df[col_name], train_col)\n",
        "        df[col_name] = col\n",
        "        df = df[df[col_name] >= 0]\n",
        "    return df"
      ],
      "metadata": {
        "id": "UOEg86N9jwqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the train and test data\n",
        "df_train = encode_data(train)\n",
        "df_test = encode_data(test, train)"
      ],
      "metadata": {
        "id": "lZ8ddOZWj1YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_train = torch.LongTensor(df_train[\"user_id\"].values)\n",
        "movies_train = torch.LongTensor(df_train[\"movie_id\"].values)\n",
        "ratings_train = torch.FloatTensor(df_train[\"rating\"].values)"
      ],
      "metadata": {
        "id": "H7-HPcYdPhZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_test = torch.LongTensor(df_test[\"user_id\"].values)\n",
        "movies_test = torch.LongTensor(df_test[\"movie_id\"].values)\n",
        "ratings_test = torch.FloatTensor(df_test[\"rating\"].values)"
      ],
      "metadata": {
        "id": "JsgooIj2UDxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_users = len(df_train.user_id.unique())\n",
        "num_items = len(df_train.movie_id.unique())\n",
        "\n",
        "print(num_users)\n",
        "print(num_items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuHu2ZlKaqHd",
        "outputId": "6e3b82bc-8379-407c-9708-44a179fd366b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6040\n",
            "3682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Factorization Model with Vector Bias"
      ],
      "metadata": {
        "id": "ky0r8Crp42-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Factorization Model Class Including Bias Vectors for Users and Items (Movies)\n",
        "class MF_bias(nn.Module):\n",
        "    def __init__(self, num_users, num_items, emb_size=64):\n",
        "\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "\n",
        "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "\n",
        "        # Initializing Embedding Weights\n",
        "        self.user_emb.weight.data.uniform_(0,0.05)\n",
        "        self.item_emb.weight.data.uniform_(0,0.05)\n",
        "\n",
        "        # Initializing Biases\n",
        "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
        "\n",
        "    # Forward Pass\n",
        "    def forward(self, u, v):\n",
        "        U = self.user_emb(u)\n",
        "        V = self.item_emb(v)\n",
        "\n",
        "        b_u = self.user_bias(u).squeeze()\n",
        "        b_v = self.item_bias(v).squeeze()\n",
        "\n",
        "        return (U*V).sum(1) +  b_u  + b_v # Adding user and item bias to dot products"
      ],
      "metadata": {
        "id": "oIcsNXmCqX16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training MF Model with Bias"
      ],
      "metadata": {
        "id": "5fHuTxgc4_6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epochs_MF_bias(model, users, items, ratings, epochs=20, lr=0.01, wd=0.0, reg_weight=0.25, unsqueeze=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd) # Initializing Adam Optimizer\n",
        "    model.train()\n",
        "    if unsqueeze:\n",
        "            ratings = ratings.unsqueeze(1)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "\n",
        "        y_hat = model(users, items)\n",
        "        mse = F.mse_loss(y_hat, ratings)\n",
        "\n",
        "        # Adding Regularization Loss to MSE\n",
        "        reg = model.user_emb(users).pow(2).sum() + model.item_emb(items).pow(2).sum()\n",
        "\n",
        "        reg += model.user_bias(users).pow(2).sum() + model.item_bias(items).pow(2).sum()\n",
        "\n",
        "        # Final loss\n",
        "        loss = mse + reg_weight * reg\n",
        "\n",
        "        # Using Gradient Descent for Optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Train Loss: %.3f\" % loss.item())\n",
        "\n",
        "        test_loss_MF_bias(model, users_test, movies_test, ratings_test, unsqueeze)"
      ],
      "metadata": {
        "id": "OrOxb0b0PTYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loss_MF_bias(model, users, items, ratings, unsqueeze=False):\n",
        "    model.eval()\n",
        "\n",
        "    if unsqueeze:\n",
        "        ratings = ratings.unsqueeze(1)\n",
        "\n",
        "    y_hat = model(users, items)\n",
        "    loss = F.mse_loss(y_hat, ratings)\n",
        "\n",
        "    print(\"Test Loss: %.3f \" % loss.item()) # MSE Loss"
      ],
      "metadata": {
        "id": "vqxQ-FewSSW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_bias_model = MF_bias(num_users, num_items, emb_size=64)"
      ],
      "metadata": {
        "id": "zfzbgPgxdNXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epochs_MF_bias(vec_bias_model, users_train, movies_train, ratings_train, epochs=50, lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q4bjqlYdhvD",
        "outputId": "37773cb5-d740-4593-a828-f4d98fa25871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 21359.203\n",
            "Test Loss: 13.802 \n",
            "Train Loss: 20100.889\n",
            "Test Loss: 13.823 \n",
            "Train Loss: 18894.787\n",
            "Test Loss: 13.842 \n",
            "Train Loss: 17740.795\n",
            "Test Loss: 13.861 \n",
            "Train Loss: 16638.176\n",
            "Test Loss: 13.878 \n",
            "Train Loss: 15586.008\n",
            "Test Loss: 13.895 \n",
            "Train Loss: 14583.400\n",
            "Test Loss: 13.910 \n",
            "Train Loss: 13629.372\n",
            "Test Loss: 13.924 \n",
            "Train Loss: 12722.795\n",
            "Test Loss: 13.937 \n",
            "Train Loss: 11862.418\n",
            "Test Loss: 13.949 \n",
            "Train Loss: 11046.966\n",
            "Test Loss: 13.960 \n",
            "Train Loss: 10275.142\n",
            "Test Loss: 13.971 \n",
            "Train Loss: 9545.615\n",
            "Test Loss: 13.980 \n",
            "Train Loss: 8856.986\n",
            "Test Loss: 13.989 \n",
            "Train Loss: 8207.823\n",
            "Test Loss: 13.997 \n",
            "Train Loss: 7596.688\n",
            "Test Loss: 14.004 \n",
            "Train Loss: 7022.145\n",
            "Test Loss: 14.011 \n",
            "Train Loss: 6482.758\n",
            "Test Loss: 14.017 \n",
            "Train Loss: 5977.076\n",
            "Test Loss: 14.022 \n",
            "Train Loss: 5503.654\n",
            "Test Loss: 14.027 \n",
            "Train Loss: 5061.057\n",
            "Test Loss: 14.031 \n",
            "Train Loss: 4647.870\n",
            "Test Loss: 14.035 \n",
            "Train Loss: 4262.696\n",
            "Test Loss: 14.038 \n",
            "Train Loss: 3904.154\n",
            "Test Loss: 14.042 \n",
            "Train Loss: 3570.889\n",
            "Test Loss: 14.044 \n",
            "Train Loss: 3261.570\n",
            "Test Loss: 14.047 \n",
            "Train Loss: 2974.902\n",
            "Test Loss: 14.049 \n",
            "Train Loss: 2709.618\n",
            "Test Loss: 14.051 \n",
            "Train Loss: 2464.490\n",
            "Test Loss: 14.053 \n",
            "Train Loss: 2238.329\n",
            "Test Loss: 14.055 \n",
            "Train Loss: 2029.986\n",
            "Test Loss: 14.056 \n",
            "Train Loss: 1838.353\n",
            "Test Loss: 14.058 \n",
            "Train Loss: 1662.364\n",
            "Test Loss: 14.059 \n",
            "Train Loss: 1500.996\n",
            "Test Loss: 14.060 \n",
            "Train Loss: 1353.273\n",
            "Test Loss: 14.061 \n",
            "Train Loss: 1218.261\n",
            "Test Loss: 14.062 \n",
            "Train Loss: 1095.071\n",
            "Test Loss: 14.062 \n",
            "Train Loss: 982.854\n",
            "Test Loss: 14.063 \n",
            "Train Loss: 880.810\n",
            "Test Loss: 14.063 \n",
            "Train Loss: 788.177\n",
            "Test Loss: 14.064 \n",
            "Train Loss: 704.239\n",
            "Test Loss: 14.064 \n",
            "Train Loss: 628.314\n",
            "Test Loss: 14.065 \n",
            "Train Loss: 559.765\n",
            "Test Loss: 14.065 \n",
            "Train Loss: 497.992\n",
            "Test Loss: 14.065 \n",
            "Train Loss: 442.429\n",
            "Test Loss: 14.065 \n",
            "Train Loss: 392.550\n",
            "Test Loss: 14.065 \n",
            "Train Loss: 347.862\n",
            "Test Loss: 14.066 \n",
            "Train Loss: 307.905\n",
            "Test Loss: 14.066 \n",
            "Train Loss: 272.251\n",
            "Test Loss: 14.066 \n",
            "Train Loss: 240.504\n",
            "Test Loss: 14.066 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Factorization Model with Confidence Bias\n",
        "\n",
        "\n",
        "\n",
        "*   **The dataset has explicit feedback in the form of movie ratings, to model it as an implicit feedback problem, I've assumed a positive rating value to be an indicator for preference of the user towards a movie, the rating value can also be thought of as the total interactions (clicks, views, etc) of the user with the movie**\n",
        "\n"
      ],
      "metadata": {
        "id": "HhOLl3cT5HL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Factorization Model Class Including Confidence Bias\n",
        "class ConfidenceMF(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=64, alpha=40.0, reg=0.01, use_bias=True):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha # Scaling factor for confidence\n",
        "        self.reg = reg\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.user_emb = torch.nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = torch.nn.Embedding(n_items, emb_size)\n",
        "\n",
        "        if use_bias:\n",
        "            self.user_bias = torch.nn.Embedding(n_users, 1)\n",
        "            self.item_bias = torch.nn.Embedding(n_items, 1)\n",
        "            self.global_bias = torch.nn.Parameter(torch.randn(1) * 0.01)\n",
        "\n",
        "        # Initializing Embedding Weights\n",
        "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
        "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
        "\n",
        "        # Initializing Biases\n",
        "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
        "\n",
        "    def forward(self, users, items, ratings):\n",
        "\n",
        "        p_ui = (ratings > 0).float() # Preference of the user, assumed to be 1 if the user has rated the movie\n",
        "        c_ui = 1.0 + self.alpha * ratings # Confidence bias term\n",
        "\n",
        "        dot = (self.user_emb(users) * self.item_emb(items)).sum(dim=1)\n",
        "\n",
        "        if self.use_bias:\n",
        "            pred = self.global_bias + self.user_bias(users).squeeze() + self.item_bias(items).squeeze() + dot # Adding global, user and item level biases\n",
        "        else:\n",
        "            pred = dot\n",
        "\n",
        "        return pred, p_ui, c_ui"
      ],
      "metadata": {
        "id": "8J53EoYXOqRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training MF Model with Confidence Bias"
      ],
      "metadata": {
        "id": "0LnEoAhM5Var"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epochs_Conf_MF(model, users, items, ratings, epochs=20, lr=0.01, wd=0.0, reg_weight=0.25, unsqueeze=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    model.train()\n",
        "    if unsqueeze:\n",
        "            ratings = ratings.unsqueeze(1)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "\n",
        "        y_hat, pref, conf = model(users, items, ratings)\n",
        "\n",
        "        diff = pref - y_hat\n",
        "        loss = (conf * diff**2).mean()\n",
        "\n",
        "        # Adding Regularization Loss to MSE\n",
        "\n",
        "        reg = model.user_emb(users).pow(2).sum() + model.item_emb(items).pow(2).sum()\n",
        "\n",
        "        reg += model.user_bias(users).pow(2).sum() + model.item_bias(items).pow(2).sum()\n",
        "\n",
        "        # Final loss\n",
        "        loss += reg_weight * reg\n",
        "\n",
        "        # Using Gradient Descent for Optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Train Loss: %.3f\" % loss.item())\n",
        "\n",
        "        test_loss_Conf_MF(model, users_test, movies_test, ratings_test, unsqueeze)"
      ],
      "metadata": {
        "id": "qvo4YUB2o-sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loss_Conf_MF(model, users, items, ratings, unsqueeze=False):\n",
        "    model.eval()\n",
        "\n",
        "    if unsqueeze:\n",
        "        ratings = ratings.unsqueeze(1)\n",
        "\n",
        "    y_hat, pref, conf = model(users, items, ratings)\n",
        "    loss = F.mse_loss(y_hat, ratings)\n",
        "\n",
        "    print(\"Test Loss: %.3f \" % loss.item()) # MSE Loss"
      ],
      "metadata": {
        "id": "N5fhvWuVrjmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_bias_model = ConfidenceMF(num_users, num_items, emb_size=64)"
      ],
      "metadata": {
        "id": "nQOcPQfitdIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epochs_Conf_MF(conf_bias_model, users_train, movies_train, ratings_train, epochs=50, lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVjt-z4Lti3H",
        "outputId": "16e13380-340a-4b7b-ae50-99f115fa559e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 21408.242\n",
            "Test Loss: 13.900 \n",
            "Train Loss: 20154.254\n",
            "Test Loss: 13.912 \n",
            "Train Loss: 18952.400\n",
            "Test Loss: 13.924 \n",
            "Train Loss: 17802.584\n",
            "Test Loss: 13.934 \n",
            "Train Loss: 16704.018\n",
            "Test Loss: 13.943 \n",
            "Train Loss: 15655.738\n",
            "Test Loss: 13.951 \n",
            "Train Loss: 14656.843\n",
            "Test Loss: 13.958 \n",
            "Train Loss: 13706.349\n",
            "Test Loss: 13.964 \n",
            "Train Loss: 12803.121\n",
            "Test Loss: 13.969 \n",
            "Train Loss: 11945.914\n",
            "Test Loss: 13.973 \n",
            "Train Loss: 11133.444\n",
            "Test Loss: 13.976 \n",
            "Train Loss: 10364.430\n",
            "Test Loss: 13.978 \n",
            "Train Loss: 9637.541\n",
            "Test Loss: 13.980 \n",
            "Train Loss: 8951.396\n",
            "Test Loss: 13.981 \n",
            "Train Loss: 8304.571\n",
            "Test Loss: 13.981 \n",
            "Train Loss: 7695.632\n",
            "Test Loss: 13.981 \n",
            "Train Loss: 7123.152\n",
            "Test Loss: 13.980 \n",
            "Train Loss: 6585.701\n",
            "Test Loss: 13.978 \n",
            "Train Loss: 6081.831\n",
            "Test Loss: 13.977 \n",
            "Train Loss: 5610.099\n",
            "Test Loss: 13.974 \n",
            "Train Loss: 5169.070\n",
            "Test Loss: 13.972 \n",
            "Train Loss: 4757.332\n",
            "Test Loss: 13.969 \n",
            "Train Loss: 4373.486\n",
            "Test Loss: 13.965 \n",
            "Train Loss: 4016.157\n",
            "Test Loss: 13.962 \n",
            "Train Loss: 3683.990\n",
            "Test Loss: 13.958 \n",
            "Train Loss: 3375.663\n",
            "Test Loss: 13.954 \n",
            "Train Loss: 3089.882\n",
            "Test Loss: 13.949 \n",
            "Train Loss: 2825.388\n",
            "Test Loss: 13.944 \n",
            "Train Loss: 2580.956\n",
            "Test Loss: 13.939 \n",
            "Train Loss: 2355.403\n",
            "Test Loss: 13.934 \n",
            "Train Loss: 2147.586\n",
            "Test Loss: 13.929 \n",
            "Train Loss: 1956.406\n",
            "Test Loss: 13.923 \n",
            "Train Loss: 1780.801\n",
            "Test Loss: 13.917 \n",
            "Train Loss: 1619.756\n",
            "Test Loss: 13.911 \n",
            "Train Loss: 1472.300\n",
            "Test Loss: 13.905 \n",
            "Train Loss: 1337.507\n",
            "Test Loss: 13.899 \n",
            "Train Loss: 1214.493\n",
            "Test Loss: 13.893 \n",
            "Train Loss: 1102.415\n",
            "Test Loss: 13.886 \n",
            "Train Loss: 1000.475\n",
            "Test Loss: 13.880 \n",
            "Train Loss: 907.916\n",
            "Test Loss: 13.873 \n",
            "Train Loss: 824.022\n",
            "Test Loss: 13.866 \n",
            "Train Loss: 748.115\n",
            "Test Loss: 13.860 \n",
            "Train Loss: 679.559\n",
            "Test Loss: 13.853 \n",
            "Train Loss: 617.755\n",
            "Test Loss: 13.846 \n",
            "Train Loss: 562.141\n",
            "Test Loss: 13.839 \n",
            "Train Loss: 512.192\n",
            "Test Loss: 13.832 \n",
            "Train Loss: 467.415\n",
            "Test Loss: 13.825 \n",
            "Train Loss: 427.352\n",
            "Test Loss: 13.818 \n",
            "Train Loss: 391.578\n",
            "Test Loss: 13.811 \n",
            "Train Loss: 359.696\n",
            "Test Loss: 13.804 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation with Precision@K\n",
        "# *The number of relevant items out of top-K recommendations*\n",
        "\n",
        "\n",
        "# Here I have assumed if the user has rated a movie,\n",
        "# then the movie is relevant to the user (does not account for ranking)"
      ],
      "metadata": {
        "id": "Zu-NeGUu8pZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_for_user(model, user_id, known_items, all_item_ids):\n",
        "    user_tensor = torch.LongTensor([user_id])\n",
        "    item_tensor = torch.LongTensor(all_item_ids)\n",
        "    user_vec = model.user_emb(user_tensor).repeat(len(all_item_ids), 1)\n",
        "    item_vecs = model.item_emb(item_tensor)\n",
        "    scores = (user_vec * item_vecs).sum(dim=1)\n",
        "    scores[known_items] = -1e10  # Excluding already seen items\n",
        "    return scores"
      ],
      "metadata": {
        "id": "6Su3rdWCAYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_recs(scores, k=10):\n",
        "    return torch.topk(scores, k=k).indices.numpy()"
      ],
      "metadata": {
        "id": "zs_HC2bnAkDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(model, df_train, df_test, k=10):\n",
        "    user_hits = []\n",
        "    user_ids = df_test['user_id'].unique()\n",
        "    all_items = np.arange(model.item_emb.num_embeddings)\n",
        "\n",
        "    for user in user_ids:\n",
        "        test_items = df_test[df_test['user_id'] == user]['movie_id'].values\n",
        "        train_items = df_train[df_train['user_id'] == user]['movie_id'].values\n",
        "\n",
        "        if len(test_items) == 0:\n",
        "            continue\n",
        "\n",
        "        scores = predict_for_user(model, user, known_items=train_items, all_item_ids=all_items)\n",
        "        top_k = get_top_k_recs(scores, k)\n",
        "\n",
        "        hits = np.isin(top_k, test_items).sum()\n",
        "        precision = hits / k\n",
        "        user_hits.append(precision)\n",
        "\n",
        "    avg_precision = np.mean(user_hits)\n",
        "    print(f\"Precision@{k}: {avg_precision:.4f}\")"
      ],
      "metadata": {
        "id": "no7nj02TAqoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_at_k(vec_bias_model, df_train, df_test, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knbcZ3nHDp0x",
        "outputId": "a51b6fde-aace-4cf4-8e74-f30f4b6e02a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@10: 0.0098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_at_k(conf_bias_model, df_train, df_test, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNKfz-81G1FH",
        "outputId": "2a7ee684-28fb-4bf0-a1ab-054e05925daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@10: 0.0103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Confidence Bias MF Model is marginally better than the Vector Bias Model under the set configurations based on Precision@K"
      ],
      "metadata": {
        "id": "i6DUfu5jJf4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation with MAP@K - Accounts for both relevance and ranking"
      ],
      "metadata": {
        "id": "6iHd60DWM5nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_at_k(model, df_train, df_test, k=10):\n",
        "    user_aps = []\n",
        "    user_ids = df_test['user_id'].unique()\n",
        "    all_items = np.arange(model.item_emb.num_embeddings)\n",
        "\n",
        "    for user in user_ids:\n",
        "        test_items = df_test[df_test['user_id'] == user]['movie_id'].values\n",
        "        train_items = df_train[df_train['user_id'] == user]['movie_id'].values\n",
        "\n",
        "        if len(test_items) == 0:\n",
        "            continue\n",
        "\n",
        "        scores = predict_for_user(model, user, known_items=train_items, all_item_ids=all_items)\n",
        "        top_k = get_top_k_recs(scores, k)\n",
        "\n",
        "        # Computing Average Precision@K\n",
        "        hits = 0\n",
        "        sum_precisions = 0.0\n",
        "        for i, item in enumerate(top_k):\n",
        "            if item in test_items:\n",
        "                hits += 1\n",
        "                precision_at_i = hits / (i + 1)\n",
        "                sum_precisions += precision_at_i\n",
        "\n",
        "        if hits > 0:\n",
        "            ap = sum_precisions / min(len(test_items), k)  # Normalize by relevant items\n",
        "        else:\n",
        "            ap = 0.0\n",
        "\n",
        "        user_aps.append(ap)\n",
        "\n",
        "    mean_ap = np.mean(user_aps)\n",
        "    print(f\"MAP@{k}: {mean_ap:.4f}\")"
      ],
      "metadata": {
        "id": "KF2OEf7sHoxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_at_k(vec_bias_model, df_train, df_test, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRYI8e7mMI-I",
        "outputId": "04ef3e8a-1cd1-4a90-c666-ed5d1fc06553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@10: 0.0033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_at_k(conf_bias_model, df_train, df_test, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmdGDcB8MNSz",
        "outputId": "d3452315-c371-4513-ff3f-b645f12878eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@10: 0.0031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here the Vector Bias MF Model is marginally better than the Confidence Bias MF Model under the set configurations based on MAP@K"
      ],
      "metadata": {
        "id": "KEuR_AO6MqPn"
      }
    }
  ]
}